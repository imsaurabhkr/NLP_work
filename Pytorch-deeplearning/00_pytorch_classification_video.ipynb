{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. PyTorch Fundamentals\n",
    "\n",
    "Resource notebook: https://www.learnpytorch.io/00_pytorch_fundamentals/\n",
    "\n",
    "If you have a question: https://github.com/mrdbourke/pytorch-deep-learning/discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pandas in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (3.9.0)\n",
      "Requirement already satisfied: filelock in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch pandas numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Make 1000 samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03,\n",
    "                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saurabhkumar/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.754246</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.756159</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.815392</td>\n",
       "      <td>0.173282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.393731</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442208</td>\n",
       "      <td>-0.896723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.479646</td>\n",
       "      <td>0.676435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.013648</td>\n",
       "      <td>0.803349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.771513</td>\n",
       "      <td>0.147760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.169322</td>\n",
       "      <td>-0.793456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.121486</td>\n",
       "      <td>1.021509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2  label\n",
       "0  0.754246  0.231481      1\n",
       "1 -0.756159  0.153259      1\n",
       "2 -0.815392  0.173282      1\n",
       "3 -0.393731  0.692883      1\n",
       "4  0.442208 -0.896723      0\n",
       "5 -0.479646  0.676435      1\n",
       "6 -0.013648  0.803349      1\n",
       "7  0.771513  0.147760      1\n",
       "8 -0.169322 -0.793456      1\n",
       "9 -0.121486  1.021509      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "circles = pd.DataFrame({\"X1\": X[:, 0], \n",
    "                        \"X2\": X[:, 1],\n",
    "                        \"label\": y})\n",
    "circles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dtype('float64'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7542,  0.2315],\n",
       "         [-0.7562,  0.1533],\n",
       "         [-0.8154,  0.1733],\n",
       "         [-0.3937,  0.6929],\n",
       "         [ 0.4422, -0.8967]]),\n",
       " tensor([1., 1., 1., 1., 0.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.2, # 0.2 = 20% of data will be test & 80% will be train\n",
    "                                                    random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 800, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model\n",
    "1. set up a device agnostic code to run on GPU if it is present\n",
    "2. constrcut a model\n",
    "3. define a loss function\n",
    "4. craete a training loop and testing loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Make device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a model\n",
    "1. subclass a nn.Module \n",
    "2. create 2 nn.Linear() layers\n",
    "3. define forward() method \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CircleModelV0(\n",
       "  (layer_1): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #1. construct a model that subclass nn.Module\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "  #2. create a 2nn. Linear layers \n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5)\n",
    "        self.layer_2= nn.Linear(in_features=5, out_features=1)\n",
    "\n",
    "   #3. define forward() method \n",
    "    def forward(self,x):\n",
    "        return self.layer_2(self.layer_1(x))         \n",
    "\n",
    "#4. create an instance of our model and send it to our target device \n",
    "model_0=CircleModelV0().to(device)\n",
    "model_0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (1): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another method to crate w model without creating a subclass using nn.sequential\n",
    "model_0= nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0146,  0.6007],\n",
       "                      [-0.6083, -0.4471],\n",
       "                      [ 0.0680,  0.1780],\n",
       "                      [-0.4916,  0.4064],\n",
       "                      [ 0.5202, -0.1994]])),\n",
       "             ('0.bias', tensor([-0.6684, -0.0918,  0.1690,  0.3457, -0.0747])),\n",
       "             ('1.weight',\n",
       "              tensor([[ 0.4268, -0.0371,  0.4377, -0.1859, -0.3906]])),\n",
       "             ('1.bias', tensor([0.3256]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up loss function \n",
    "1. loss_fn-> two option \n",
    " a. nn.BCELoss-> requires input to go through sigmoifd activation function and then insert into bce loss\n",
    " b. nn.BCEWithLogitLoss = it combines sigmoid activation function along with bce loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn= nn.BCELoss();\n",
    "loss_fn=nn.BCEWithLogitsLoss();\n",
    "optimizer= torch.optim.SGD(params=model_0.parameters(),\n",
    "                           lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy \n",
    "def accuracy_fn(y_true,y_pred):\n",
    "    correct= torch.eq(y_true,y_pred).sum().item()\n",
    "    acc=(correct/len(y_pred))*100;\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the model\n",
    "build a training loop with follow steps:\n",
    "\n",
    "1. forward pass\n",
    "2. calculate the loss\n",
    "3. optimizer zero grad\n",
    "4. loss backward(backpropagation)\n",
    "5. optimizer step(gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Going from raw logits-> prediction probabilities -> prediction label\n",
    "\n",
    "initially our model output is to be raw **logits**\n",
    " we have to convert **logits** into prediction probabilities by passsing them to activation function (eg: sigmoid for binary classification and softmax for multi)\n",
    "\n",
    " convert our model prediction prob into **prediction label** by either rounding or taking **argmax()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3483],\n",
       "        [ 0.4207],\n",
       "        [ 0.0176],\n",
       "        [ 0.4426],\n",
       "        [-0.2065]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "  y_logits = model_0(X_test.to(device))[:5]\n",
    "y_logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# u can see our model raw ouput or logits and our real y_test ,,\n",
    " so we have to first convert them into probabilities and then assign them some labelling based on prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5862],\n",
       "        [0.6036],\n",
       "        [0.5044],\n",
       "        [0.6089],\n",
       "        [0.4486]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use sigmoid actiavtion function on our model logits to convert into prediction probabilities\n",
    "y_pred_probs=torch.sigmoid(y_logits)\n",
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(y_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "\n",
      "  tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# find the predicted label\n",
    "y_preds=torch.round(y_pred_probs)\n",
    "\n",
    "# in full\n",
    "y_pred_lables=torch.round(torch.sigmoid(model_0(X_test)[:5]))\n",
    "\n",
    "print(f\"\\n {y_preds}\")\n",
    "print(f\"\\n  {y_pred_lables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2  Build training loop and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs=100\n",
    "\n",
    "# put data to target device \n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# build training loop and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ## training \n",
    "    model_0.train()\n",
    "\n",
    "    ## 1. forward pass\n",
    "    y_logits=model_0(X_train).squeeze()\n",
    "    y_pred=torch.round(torch.sigmoid(y_logits))\n",
    "    # logit-> prediction_prob -> prediction_label\n",
    "\n",
    "    ## 2. Calculate loss/ accuracy\n",
    "    # loss=loss_fn(torch.sigmoid(y_logits),y_train)\n",
    "    # nn.BCELoss expects prediction probabilities as input\n",
    "    loss= loss_fn(y_logits,y_train)\n",
    "    # nn.BCEWithLogitLoss expect raw logits as input \n",
    "    acc=accuracy_fn(y_true=y_train,y_pred=y_pred)\n",
    "     \n",
    "    #  observe: loss-> first pred then train \n",
    "    #           acc-> first train then pred (it is standard notation for accuracy refer documentation)\n",
    "\n",
    "    ## 3. optimizer zero grad \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ##4. loss_backward(backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    ##5. optimizer step(gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits=model_0(X_test).squeeze()\n",
    "        test_pred=torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        #2. calculate test loss/acc\n",
    "        test_loss=loss_fn(test_logits,y_test)\n",
    "        test_acc=accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "#print \n",
    "if epoch % 10 == 0:\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.69336, Acc: 50.62% | Test loss: 0.69710, Test acc: 45.50%\n",
      "Epoch: 10 | Loss: 0.69328, Acc: 50.75% | Test loss: 0.69679, Test acc: 46.50%\n",
      "Epoch: 20 | Loss: 0.69321, Acc: 50.88% | Test loss: 0.69652, Test acc: 47.50%\n",
      "Epoch: 30 | Loss: 0.69316, Acc: 50.88% | Test loss: 0.69629, Test acc: 47.50%\n",
      "Epoch: 40 | Loss: 0.69312, Acc: 50.88% | Test loss: 0.69609, Test acc: 47.00%\n",
      "Epoch: 50 | Loss: 0.69309, Acc: 50.88% | Test loss: 0.69592, Test acc: 47.00%\n",
      "Epoch: 60 | Loss: 0.69307, Acc: 51.12% | Test loss: 0.69577, Test acc: 46.50%\n",
      "Epoch: 70 | Loss: 0.69305, Acc: 51.12% | Test loss: 0.69564, Test acc: 47.50%\n",
      "Epoch: 80 | Loss: 0.69303, Acc: 51.25% | Test loss: 0.69553, Test acc: 47.00%\n",
      "Epoch: 90 | Loss: 0.69302, Acc: 51.12% | Test loss: 0.69543, Test acc: 47.50%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42) \n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device \n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "  ### Training\n",
    "  model_0.train()\n",
    "\n",
    "  # 1. Forward pass\n",
    "  y_logits = model_0(X_train).squeeze()\n",
    "  y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
    "\n",
    "  # 2. Calculate loss/accuracy\n",
    "  # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction probabilities as input\n",
    "  #                y_train)\n",
    "  loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input\n",
    "                 y_train)\n",
    "  acc = accuracy_fn(y_true=y_train, \n",
    "                    y_pred=y_pred)\n",
    "  \n",
    "  # 3. Optimizer zero grad\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # 4. Loss backward (backpropagation)\n",
    "  loss.backward()\n",
    "\n",
    "  # 5. Optimizer step (gradient descent)\n",
    "  optimizer.step() \n",
    "\n",
    "  ### Testing\n",
    "  model_0.eval()\n",
    "  with torch.inference_mode():\n",
    "    # 1. Forward pass \n",
    "    test_logits = model_0(X_test).squeeze()\n",
    "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "    # 2. Calculate test loss/acc\n",
    "    test_loss = loss_fn(test_logits,\n",
    "                        y_test)\n",
    "    test_acc = accuracy_fn(y_true=y_test,\n",
    "                           y_pred=test_pred)\n",
    "  \n",
    "  # Print out what's happenin'\n",
    "  if epoch % 10 == 0:\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How to improve the model\n",
    "\n",
    "1. Add more layer\n",
    "2. Add more hidden unit ->go from 5 hidden unit to 10hidden unit \n",
    "3. fit for longer (more epoch)\n",
    "4. change the activation function\n",
    "5. change the learning rate\n",
    "6. change the loss function \n",
    "\n",
    "these option are from model perspective bcoz they deal with model not data know as **hyperparameter**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve accuracy greater than 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlobModel(\n",
       "  (linear_layer_stack): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=8, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BlobModel(nn.Module):\n",
    "    def __init__(self,input_features,output_features,hidden_units=8):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack=nn.Sequential(\n",
    "            nn.Linear(in_features=input_features,out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units,out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units,out_features=output_features)\n",
    "\n",
    "        )\n",
    "        def forward(self,x):\n",
    "            return self.linear_layer_stack(x)\n",
    "\n",
    "#instance of model\n",
    "model_4=BlobModel(input_features=2,\n",
    "                  output_features=4,\n",
    "                  hidden_units=8).to(device)\n",
    "model_4                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a loss function and optimizer for multiclass model\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer=torch.optim.SGD(params=model_4.parameters(),\n",
    "                          lr=0.1\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here also same pattern , \n",
    "# first raw logits -> pred probabilitt -> pred lablel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training and testing loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model_4.train()\n",
    "\n",
    "    y_logits=model_4(x_blob_train)\n",
    "    y_pred=torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "    loss=loss_fn(y_logits,y_blob_train)\n",
    "    acc=accuracy_fn(y_true=y_blob_train, \n",
    "                    y_pred=y_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few more classification metrics\n",
    "1. accuracy : it is good but only for balanced dataset like for \n",
    "one class we have 500 point , for other 600 points in this way .. but suppose we have 5 datapoint for one class and 2000 data point for other class , in such type of imbalanced data , accuracy is not a good option .. that why we use precision and recall \n",
    "\n",
    "Refer this link very beautiful blog for intuition :\n",
    "https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
